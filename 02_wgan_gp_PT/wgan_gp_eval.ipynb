{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b4860b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# ------------ Ejemplo de uso ------------\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# 1) Importa tu encoder (debes exponer json_a_vector que devuelva el vector en memoria)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEncoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson_a_img\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m json_a_vector, FUNCIONES_API  \u001b[38;5;66;03m# <-- ajusta el import a tu proyecto\u001b[39;00m\n\u001b[32m    160\u001b[39m     EXPECTED_LEN = CONT_DIM + \u001b[38;5;28mlen\u001b[39m(FUNCIONES_API)\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# 2) Reales: lee desde un directorio con JSON\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'Encoder'"
     ]
    }
   ],
   "source": [
    "import os, glob, json\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ==== CONFIG BÁSICA (ajusta si cambias tu encoder) ====\n",
    "CONT_DIM = 19  # 4 (fam_name_norm,size_norm,timestamp_norm,entropy_norm) + 4secciones*3(entropía,virtual_size,raw_size) + 3 extra(entropía,virtual_size,raw_size) = 19\n",
    "RANDOM_STATE = 42\n",
    "MAX_SAMPLES = 2000  # para hacerlo veloz\n",
    "Z_DIM = 100\n",
    "# Elegimos features continuos \"clave\" para KS (índices dentro de los 19 continuos):\n",
    "KS_FEATURES_IDX = [\n",
    "    1,  # file_size_norm\n",
    "    2,  # timestamp_norm\n",
    "    3,  # entropy global\n",
    "    4, 5, 6,   # .text: ent, vsize, rsize\n",
    "    7, 8, 9    # .idata: ent, vsize, rsize\n",
    "]\n",
    "\n",
    "# ------------ Helpers mínimos ------------\n",
    "def ensure_binary_01(X: np.ndarray, cont_dim: int = CONT_DIM) -> np.ndarray:\n",
    "    \"\"\"Si los binarios vienen en {-1,1}, mapea a {0,1}.\"\"\"\n",
    "    X = X.copy()\n",
    "    if X.shape[1] > cont_dim:\n",
    "        Xb = X[:, cont_dim:]\n",
    "        if Xb.min() < 0.0:\n",
    "            X[:, cont_dim:] = (Xb + 1.0) / 2.0\n",
    "    return X\n",
    "\n",
    "def subset_balanced(Xr: np.ndarray, Xs: np.ndarray, max_n: int = MAX_SAMPLES) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    n = min(len(Xr), len(Xs), max_n)\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    idx_r = rng.choice(len(Xr), size=n, replace=False) if len(Xr) > n else np.arange(len(Xr))\n",
    "    idx_s = rng.choice(len(Xs), size=n, replace=False) if len(Xs) > n else np.arange(len(Xs))\n",
    "    return Xr[idx_r], Xs[idx_s]\n",
    "\n",
    "def imgs_to_vectors(gen_imgs: np.ndarray, expected_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convierte imágenes [-1,1] a vectores [0,1] con flatten.\n",
    "    Recorta/paddea a expected_len = 19 + N.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for img in gen_imgs:\n",
    "        vec01 = (img + 1.0) / 2.0\n",
    "        v = vec01.astype(np.float32).flatten()\n",
    "        if len(v) < expected_len:\n",
    "            v = np.pad(v, (0, expected_len - len(v)))\n",
    "        elif len(v) > expected_len:\n",
    "            v = v[:expected_len]\n",
    "        out.append(v)\n",
    "    return np.array(out, dtype=np.float32)\n",
    "\n",
    "# ------------ Métricas smoke ------------\n",
    "def c2st_auc(Xr_all: np.ndarray, Xs_all: np.ndarray) -> float:\n",
    "    X = np.vstack([Xr_all, Xs_all])\n",
    "    y = np.hstack([np.ones(len(Xr_all)), np.zeros(len(Xs_all))])\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
    "    clf = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    proba = clf.predict_proba(X_te)[:, 1]\n",
    "    return float(roc_auc_score(y_te, proba))\n",
    "\n",
    "def jensen_shannon_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"JSD(P||Q) simple sobre frecuencias columna (binarias).\"\"\"\n",
    "    eps = 1e-8\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    q = np.clip(q, eps, 1 - eps)\n",
    "    m = 0.5 * (p + q)\n",
    "    kl_pm = np.sum(p * (np.log(p) - np.log(m)))\n",
    "    kl_qm = np.sum(q * (np.log(q) - np.log(m)))\n",
    "    return float(0.5 * (kl_pm + kl_qm))\n",
    "\n",
    "def jsd_imports(Xr: np.ndarray, Xs: np.ndarray, cont_dim: int = CONT_DIM) -> float:\n",
    "    if Xr.shape[1] <= cont_dim:\n",
    "        return float(\"nan\")\n",
    "    pr = Xr[:, cont_dim:].mean(axis=0)\n",
    "    ps = Xs[:, cont_dim:].mean(axis=0)\n",
    "    return jensen_shannon_divergence(pr, ps)\n",
    "\n",
    "def ks_summary(Xr: np.ndarray, Xs: np.ndarray, features_idx: List[int]) -> Dict[str, Any]:\n",
    "    from scipy.stats import ks_2samp\n",
    "    rows, pvals = [], []\n",
    "    for j in features_idx:\n",
    "        stat, p = ks_2samp(Xr[:, j], Xs[:, j])\n",
    "        rows.append({\"feature_idx\": j, \"KS_stat\": float(stat), \"p_value\": float(p)})\n",
    "        pvals.append(p)\n",
    "    return {\n",
    "        \"by_feature\": rows,\n",
    "        \"fraction_p_gt_0_05\": float(np.mean(np.array(pvals) > 0.05))\n",
    "    }\n",
    "\n",
    "# ------------ Carga de reales (usa tu json_a_vector) ------------\n",
    "def collect_real_vectors(json_dir: str, expected_len: int, json_a_vector_func) -> np.ndarray:\n",
    "    X = []\n",
    "    for p in glob.glob(os.path.join(json_dir, \"*.json\")):\n",
    "        vec = json_a_vector_func(p)  # ← tu función debe devolver np.array 1D\n",
    "        v = np.asarray(vec, dtype=np.float32)\n",
    "        if len(v) < expected_len:\n",
    "            v = np.pad(v, (0, expected_len - len(v)))\n",
    "        elif len(v) > expected_len:\n",
    "            v = v[:expected_len]\n",
    "        X.append(v)\n",
    "    if not X:\n",
    "        raise RuntimeError(f\"No se encontraron JSON en: {json_dir}\")\n",
    "    return np.stack(X, axis=0)\n",
    "\n",
    "# ------------ Función principal (smoke test) ------------\n",
    "def smoke_test_gan(\n",
    "    X_real: np.ndarray,\n",
    "    X_syn: np.ndarray,\n",
    "    cont_dim: int = CONT_DIM\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Recibe vectores reales y sintéticos (mismo dim), hace:\n",
    "      - subset balanceado\n",
    "      - estandariza solo continuos con fit en reales\n",
    "      - C2ST AUC, JSD imports, KS continuos clave\n",
    "    \"\"\"\n",
    "    # asegurar binarios en 0/1\n",
    "    X_real = ensure_binary_01(X_real, cont_dim)\n",
    "    X_syn  = ensure_binary_01(X_syn, cont_dim)\n",
    "\n",
    "    # subset balanceado y pequeño (rápido)\n",
    "    Xr, Xs = subset_balanced(X_real, X_syn, MAX_SAMPLES)\n",
    "\n",
    "    # estandarizar SOLO continuos con fit en reales\n",
    "    scaler = StandardScaler()\n",
    "    Xr_cont = scaler.fit_transform(Xr[:, :cont_dim])\n",
    "    Xs_cont = scaler.transform(Xs[:, :cont_dim])\n",
    "\n",
    "    # concatenar binarios sin escalar\n",
    "    if Xr.shape[1] > cont_dim:\n",
    "        Xr_all = np.concatenate([Xr_cont, Xr[:, cont_dim:]], axis=1)\n",
    "        Xs_all = np.concatenate([Xs_cont, Xs[:, cont_dim:]], axis=1)\n",
    "    else:\n",
    "        Xr_all, Xs_all = Xr_cont, Xs_cont\n",
    "\n",
    "    # métricas\n",
    "    auc = c2st_auc(Xr_all, Xs_all)\n",
    "    jsd = jsd_imports(Xr, Xs, cont_dim=cont_dim)\n",
    "    ks  = ks_summary(Xr[:, :cont_dim], Xs[:, :cont_dim], KS_FEATURES_IDX)\n",
    "\n",
    "    return {\n",
    "        \"C2ST_auc\": auc,                          # ideal ≈ 0.5–0.6                     C2ST (clasificador Real vs Sintético)\n",
    "        \"JSD_imports\": jsd,                       # ideal < ~0.05                       JSD en imports (binarios)\n",
    "        \"KS_fraction_p>0.05\": ks[\"fraction_p_gt_0_05\"],  # ideal ≥ ~0.7                 KS en continuos\n",
    "        \"KS_details\": ks[\"by_feature\"],\n",
    "        \"samples_used_per_set\": len(Xr)\n",
    "    }\n",
    "\n",
    "# ------------ Ejemplo de uso ------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Importa tu encoder (debes exponer json_a_vector que devuelva el vector en memoria)\n",
    "    from Encoder.json_a_img import json_a_vector, FUNCIONES_API  # <-- ajusta el import a tu proyecto\n",
    "\n",
    "    EXPECTED_LEN = CONT_DIM + len(FUNCIONES_API)\n",
    "\n",
    "    # 2) Reales: lee desde un directorio con JSON\n",
    "    JSON_DIR = \"./muestras_malware/json\"\n",
    "    X_real = collect_real_vectors(JSON_DIR, EXPECTED_LEN, json_a_vector)\n",
    "\n",
    "    # 3) Sintéticos: conviértelas desde gen_imgs ([-1,1]) a vectores\n",
    "    generator = load_model(\"./models/generator.keras\")\n",
    "    z_sample = np.random.normal(size=(584, Z_DIM))\n",
    "    gen_imgs = generator.predict(z_sample)\n",
    "    X_syn = imgs_to_vectors(gen_imgs, EXPECTED_LEN)\n",
    "\n",
    "    # 4) Ejecutar smoke test\n",
    "    report = smoke_test_gan(X_real, X_syn, cont_dim=CONT_DIM)\n",
    "    import pprint; pprint.pprint(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
